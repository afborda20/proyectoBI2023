{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5470a4e1-29b6-4af5-87de-2dfad5388afb",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3aecf75-2af8-4f8a-90aa-50535dab7f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: num2words in c:\\users\\andre\\anaconda3\\lib\\site-packages (0.5.12)\n",
      "Requirement already satisfied: docopt>=0.6.2 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from num2words) (0.6.2)\n",
      "Requirement already satisfied: langdetect in c:\\users\\andre\\anaconda3\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\andre\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: stanza in c:\\users\\andre\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: emoji in c:\\users\\andre\\anaconda3\\lib\\site-packages (from stanza) (2.8.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from stanza) (2.1.0)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from stanza) (4.24.4)\n",
      "Requirement already satisfied: requests in c:\\users\\andre\\anaconda3\\lib\\site-packages (from stanza) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\andre\\anaconda3\\lib\\site-packages (from stanza) (4.64.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\andre\\anaconda3\\lib\\site-packages (from stanza) (1.21.5)\n",
      "Requirement already satisfied: sympy in c:\\users\\andre\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (1.10.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\andre\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (4.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\andre\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (3.6.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\andre\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (2022.7.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\andre\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (2.11.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from requests->stanza) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from requests->stanza) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from requests->stanza) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from requests->stanza) (2022.9.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\andre\\anaconda3\\lib\\site-packages (from tqdm->stanza) (0.4.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.3.0->stanza) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\andre\\anaconda3\\lib\\site-packages (from sympy->torch>=1.3.0->stanza) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "#Instalaciones\n",
    "\n",
    "#Libreria para remplazar numeros por palabras\n",
    "!pip install num2words\n",
    "#Instalar  pandas profiler\n",
    "#!pip install pandas-profiling==2.7.1\n",
    "#deteccion de lenguaje para eliminar entradas que no esten en español\n",
    "!pip install langdetect\n",
    "#Procesamientno de lenguaje natural en español\n",
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cdce2af-087e-4934-8bb7-59e377714aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a6de741a45439eba699b822d52c591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:02:30 INFO: Downloading default packages for language: es (Spanish) ...\n",
      "2023-10-26 12:02:31 INFO: File exists: C:\\Users\\andre\\stanza_resources\\es\\default.zip\n",
      "2023-10-26 12:02:35 INFO: Finished downloading models and saved to C:\\Users\\andre\\stanza_resources.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#imports para procesamiento de texto\n",
    "\n",
    "#tokenizacion y lematizacion\n",
    "import stanza\n",
    "#Para integrar pasos de la limpieza adicionales\n",
    "from stanza.pipeline.processor import Processor, register_processor\n",
    "#paquete español\n",
    "stanza.download('es')\n",
    "\n",
    "#Para manejo de numeros, singluares, plurarles en lenguaje\n",
    "from num2words import num2words\n",
    "#Deteccion de lenguaje\n",
    "from langdetect import detect\n",
    "# librería Natural Language Toolkit, usada para trabajar con textos\n",
    "import nltk\n",
    "# Punkt permite separar un texto en frases.\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Operaciones con expresiones regulares y unicode\n",
    "import re, string, unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f06c1e-011d-4841-9a42-9ecb2eb612a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_2256\\122007551.py:5: DeprecationWarning: `import pandas_profiling` is going to be deprecated by April 1st. Please use `import ydata_profiling` instead.\n",
      "  from pandas_profiling import ProfileReport\n"
     ]
    }
   ],
   "source": [
    "#Imports generales para analisis de datos y ML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from pandas_profiling import ProfileReport\n",
    "import statistics\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f49cc60-c1c5-43a5-badb-b889d440b08b",
   "metadata": {},
   "source": [
    "## 1. Lectura de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa20de74-4d8d-4527-8371-64e8b9fe2e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Textos_espanol</th>\n",
       "      <th>sdg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Es importante destacar que, en un año de sequí...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hay una gran cantidad de literatura sobre Aust...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Los procesos de descentralización, emprendidos...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Esto puede tener consecuencias sustanciales pa...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La función de beneficio también incorpora pará...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Textos_espanol  sdg\n",
       "0  Es importante destacar que, en un año de sequí...    6\n",
       "1  Hay una gran cantidad de literatura sobre Aust...    6\n",
       "2  Los procesos de descentralización, emprendidos...    6\n",
       "3  Esto puede tener consecuencias sustanciales pa...    6\n",
       "4  La función de beneficio también incorpora pará...    6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lectura de los datos.\n",
    "df_ODS = pd.read_excel(\"cat_6716.xlsx\")\n",
    "\n",
    "df_ODS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a6e00a-39d4-4ba5-9684-9b0f9655ce40",
   "metadata": {},
   "source": [
    "## 2. Construcción del pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "418cdb3e-26b4-4808-adb0-4d20ba248a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33235196-9dae-4b44-96e6-ed67c6f4912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def init(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        X_processed = self.customPreprocessing(X)\n",
    "        #Retornar los datos\n",
    "        return X_processed\n",
    "    \n",
    "    #Remplaza los numeros por su representacion en palabras\n",
    "    def replace_numbers(self, words):\n",
    "        \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if word.isdigit():\n",
    "                new_word = num2words(word, lang='es')\n",
    "                new_words.append(new_word)\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        return new_words\n",
    "    #Remueve todo caracter no latino (conserva espacios y numeros)\n",
    "    def remove_nonlatin(self, words):\n",
    "      new_words = []\n",
    "      for word in words:\n",
    "        new_word = ''\n",
    "        for ch in word:\n",
    "          if unicodedata.name(ch).startswith(('LATIN', 'DIGIT', 'SPACE')):\n",
    "            new_word += ch\n",
    "        new_words.append(new_word)\n",
    "      return new_words\n",
    "\n",
    "    #Remueve palabras comunes que no aportan informacion\n",
    "    def remove_stopwords(self, words):\n",
    "        \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        s = set(stopwords.words('spanish'))\n",
    "        for word in words:\n",
    "            if word not in s:\n",
    "                new_words.append(word)\n",
    "        return new_words\n",
    "\n",
    "    #Remueve puntuacion\n",
    "    def remove_punctuation(self, words):\n",
    "        \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "        new_words = ''\n",
    "        for word in words:\n",
    "                new_words += re.sub(r'[^\\w\\s]', ' ', word)\n",
    "        return new_words\n",
    "\n",
    "     #Procesamiento de cada review usando stanza\n",
    "    def tokenLemma(self, data):\n",
    "      data['words'] = data['Textos_espanol'].apply(self.remove_punctuation)\n",
    "      #Creamos un pipeline para tokenizacion y lematizacion\n",
    "      nlp = stanza.Pipeline('es', processors = 'tokenize,mwt,pos,lemma', use_gpu=True)\n",
    "      in_docs = [stanza.Document([], text=d) for d in data.words]\n",
    "      return nlp(in_docs)\n",
    "\n",
    "    #Funcion secundaria para procesar cada token\n",
    "    def procesamientoPalabras(self, words):\n",
    "        words = self.remove_nonlatin(words)\n",
    "        words = self.replace_numbers(words)\n",
    "        words = self.remove_stopwords(words)\n",
    "        return words\n",
    "\n",
    "    #Funcion principal para el pre-procesamiento\n",
    "    def customPreprocessing(self, data):\n",
    "        out_docs = self.tokenLemma(data)\n",
    "        palabras = []\n",
    "\n",
    "        for doc in out_docs:\n",
    "            reviewAct = []\n",
    "            for sentence in doc.sentences:\n",
    "              for word in sentence.words:\n",
    "                if(word.pos != 'PUNCT' and word.pos != 'SYM'):\n",
    "                  reviewAct.append(word.lemma.lower())\n",
    "            palabras.append(reviewAct)\n",
    "        \n",
    "        data['words'] = palabras\n",
    "        data['words'] = data['words'].apply(self.procesamientoPalabras)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3904bff4-4c78-492f-8627-b08975ab84b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import style\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import RepeatedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0534e65-20c8-4f28-8056-ba37db1773ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRegression(BaseEstimator, TransformerMixin):\n",
    "    def init(self):\n",
    "        self.model = None\n",
    "        self.params = None\n",
    "        self.accuracy = None\n",
    "        self.vec = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "                \n",
    "        X['words'] = X['words'].apply(lambda x: ' '.join(map(str, x)))\n",
    "        \n",
    "        #Separación de los datos en conjunto de test y train\n",
    "        X = X.drop('Textos_espanol', axis = 1)\n",
    "        df_train, df_test = sklearn.model_selection.train_test_split(X, test_size=0.2, random_state=0)\n",
    "\n",
    "        X_train = df_train['words']\n",
    "        y_train = df_train['sdg']\n",
    "\n",
    "        X_test = df_test['words']\n",
    "        y_test = df_test['sdg']\n",
    "        \n",
    "        #Vectorizar los datos con Tfid\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        train_vectors = vectorizer.fit_transform(X_train)\n",
    "        test_vectors = vectorizer.transform(X_test)\n",
    "        \n",
    "        self.vec = vectorizer\n",
    "        \n",
    "        parameters = {\n",
    "            'penalty' : ['l1','l2', 'elasticnet', None], \n",
    "            'C'       : np.logspace(-10,10,3),\n",
    "            'solver'  : ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "        }\n",
    "        \n",
    "        metrica = RepeatedKFold(n_splits=20, n_repeats= 10, random_state=0)\n",
    "\n",
    "        logreg = LogisticRegression()\n",
    "        modelo = GridSearchCV(logreg, param_grid = parameters, scoring='accuracy', cv=metrica, n_jobs=-1)  \n",
    "        \n",
    "        modelo.fit(train_vectors,y_train)\n",
    "        self.params = modelo.best_params_\n",
    "        self.accuracy = modelo.best_score_\n",
    "        modelo_optimo = modelo.best_estimator_\n",
    "                \n",
    "        self.model = modelo_optimo\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):        \n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5936a6e2-4967-4f93-8928-e174a370db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_preprocessor = CustomPreprocessor()\n",
    "custom_regression = CustomRegression()\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"processing\", custom_preprocessor),\n",
    "        (\"model\", custom_regression)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d57ebee-28f6-456e-83b9-592c6313c913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:02:41 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a55be643814d12b2907f988716b990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 12:02:42 INFO: Loading these models for language: es (Spanish):\n",
      "===============================\n",
      "| Processor | Package         |\n",
      "-------------------------------\n",
      "| tokenize  | ancora          |\n",
      "| mwt       | ancora          |\n",
      "| pos       | ancora_charlm   |\n",
      "| lemma     | ancora_nocharlm |\n",
      "===============================\n",
      "\n",
      "2023-10-26 12:02:42 WARNING: GPU requested, but is not available!\n",
      "2023-10-26 12:02:42 INFO: Using device: cpu\n",
      "2023-10-26 12:02:42 INFO: Loading: tokenize\n",
      "2023-10-26 12:02:43 INFO: Loading: mwt\n",
      "2023-10-26 12:02:43 INFO: Loading: pos\n",
      "2023-10-26 12:02:43 INFO: Loading: lemma\n",
      "2023-10-26 12:02:43 INFO: Done loading processors!\n",
      "C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "4800 fits failed out of a total of 7200.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1800 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 441, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\andre\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.31791667 0.33070833 0.61670833 0.61670833\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.95595833 0.98433333 0.98433333 0.98333333\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.94145833 0.98683333 0.98483333 0.986875\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('processing', CustomPreprocessor()),\n",
       "                ('model', CustomRegression())])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajusta el modelo en tus datos transformados\n",
    "pipeline.fit(df_ODS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10b6d2e4-bd91-4e86-813a-090034c079fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = pipeline[\"model\"].model\n",
    "vectorizer = pipeline[\"model\"].vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e00209-eb6a-429d-ab83-b0b24eca4ebe",
   "metadata": {},
   "source": [
    "## 3. Testear el funcionamiento del pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4d173b5-99c0-446f-ae84-008af369dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se cargan los datos no etiquetados\n",
    "df_test = pd.read_csv(\"df_procesado_predecir.csv\", sep = \";\", encoding='utf-8')\n",
    "import ast\n",
    "df_test['words'] = df_test['words'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c13df99-7a94-40eb-ae41-fa17a48d2ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos los tokens nuevamente en strings\n",
    "df_test['words'] = df_test['words'].apply(lambda x: ' '.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdfb8094-54e3-4860-9f1f-ce2830512616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separar datos para predecir\n",
    "df_test = df_test.drop('Textos_espanol', axis = 1)\n",
    "\n",
    "X_predict = df_test['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48e7e247-b36d-4bb5-8c9a-789288e67b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      introducción estructuras derecho penal dos est...\n",
       "1      agua subterráneo haber debatir contexto tarifi...\n",
       "2      presente contribución evalúar jurisprudencia t...\n",
       "3      embargo crédito fiscal expirar dos mil doce we...\n",
       "4      estudio explorar actitud comportamiento percep...\n",
       "                             ...                        \n",
       "975    artículo explorar historia impacto acción afir...\n",
       "976    mientras alguno dar mayor énfasis manipulación...\n",
       "977    innovación importante garantizar soporte efect...\n",
       "978    salvador continuar luchar nivel elevado violen...\n",
       "979    poder reflejar bajo conciencia papel jugar rec...\n",
       "Name: words, Length: 980, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a33cf73-fe84-40eb-a9b1-146d7d4884e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "449162ca-83bb-4f43-905d-c231878a5e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorizar los datos con Tfid\n",
    "vectors = vectorizer.transform(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef3ec92-9c3c-45e0-aa2e-54e7255ff4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16,  6, 16,  7, 16, 16,  6, 16, 16,  7, 16,  7,  7, 16, 16,  6, 16,\n",
       "        6, 16, 16, 16, 16, 16,  6,  7,  7, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "       16,  7, 16, 16,  7, 16,  7, 16, 16,  7, 16, 16, 16, 16, 16, 16, 16,\n",
       "       16, 16,  6, 16,  7, 16, 16, 16, 16,  7, 16, 16,  7,  7, 16,  6,  7,\n",
       "       16, 16,  6, 16,  7, 16, 16,  7, 16,  7, 16, 16, 16,  6, 16, 16,  7,\n",
       "       16, 16,  7, 16, 16, 16, 16, 16, 16,  7,  7, 16,  6,  7,  7, 16,  7,\n",
       "       16, 16, 16, 16,  7, 16, 16,  7, 16,  7,  6, 16, 16, 16, 16, 16, 16,\n",
       "        7,  7,  7, 16, 16, 16, 16, 16, 16, 16,  7,  7,  7,  7, 16,  7,  7,\n",
       "        7, 16,  6, 16, 16, 16,  7, 16, 16,  7, 16, 16,  6, 16, 16, 16, 16,\n",
       "        7, 16, 16, 16, 16,  7, 16, 16, 16,  7, 16, 16, 16,  6, 16, 16,  7,\n",
       "        6,  6, 16,  6, 16, 16, 16,  7, 16,  7, 16, 16, 16, 16, 16, 16,  7,\n",
       "        7, 16, 16, 16,  6, 16,  6,  7, 16,  7, 16, 16, 16,  6, 16, 16,  7,\n",
       "        7, 16, 16, 16, 16, 16,  6,  6, 16, 16, 16, 16,  7, 16, 16, 16, 16,\n",
       "       16, 16,  6, 16,  7, 16, 16,  7,  7, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "       16, 16,  6, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,  7,  7,  6,\n",
       "        7,  7,  7, 16,  7, 16, 16, 16, 16, 16,  7,  7, 16,  6, 16, 16,  7,\n",
       "       16,  7, 16, 16,  7,  7, 16, 16, 16, 16, 16, 16,  7, 16, 16, 16, 16,\n",
       "        7, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,  7, 16,  6, 16,  6,\n",
       "       16,  6, 16, 16, 16, 16, 16, 16, 16,  7,  7,  7,  6, 16, 16,  7, 16,\n",
       "       16,  7, 16, 16,  7, 16,  7, 16, 16, 16, 16, 16,  7, 16, 16, 16, 16,\n",
       "       16, 16, 16, 16,  7, 16, 16, 16, 16, 16, 16, 16,  7, 16, 16, 16, 16,\n",
       "        7,  7, 16,  7, 16, 16, 16,  7, 16, 16, 16,  7, 16, 16, 16, 16, 16,\n",
       "        7, 16,  7, 16, 16, 16, 16, 16,  6, 16, 16,  7,  6,  7, 16, 16, 16,\n",
       "       16, 16, 16,  6, 16, 16,  6,  7, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        6, 16,  7, 16,  7,  7, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        7, 16,  7, 16, 16,  7, 16, 16, 16, 16, 16,  6,  6,  7, 16, 16, 16,\n",
       "       16,  7,  7, 16, 16,  7, 16, 16, 16, 16, 16, 16, 16, 16, 16,  7, 16,\n",
       "       16,  7, 16, 16, 16, 16, 16, 16, 16,  7, 16, 16, 16, 16,  7, 16,  7,\n",
       "        7, 16, 16, 16,  7, 16, 16, 16, 16, 16,  7,  7,  7, 16, 16, 16, 16,\n",
       "        6, 16, 16, 16,  6,  6, 16,  6,  7, 16,  6, 16, 16,  7, 16, 16, 16,\n",
       "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,  7, 16,  7, 16,  7, 16,\n",
       "       16, 16, 16, 16, 16, 16, 16, 16,  7, 16,  6, 16, 16, 16, 16,  7,  7,\n",
       "        7, 16,  7, 16, 16, 16,  7, 16, 16, 16,  7, 16, 16, 16,  7, 16, 16,\n",
       "        7, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,  7, 16, 16, 16, 16,\n",
       "        7,  7,  7, 16, 16,  7, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,  7,\n",
       "        7, 16,  7, 16, 16, 16, 16, 16, 16,  7,  7,  7, 16, 16, 16, 16, 16,\n",
       "       16, 16, 16, 16, 16, 16,  7, 16,  6, 16,  7, 16,  7,  7,  7,  7, 16,\n",
       "       16,  6,  6, 16,  7, 16, 16, 16, 16, 16,  7, 16, 16, 16, 16, 16,  7,\n",
       "       16, 16,  7, 16,  7, 16, 16, 16, 16, 16, 16, 16, 16,  6, 16, 16, 16,\n",
       "        7, 16, 16,  7,  7, 16,  7, 16, 16, 16,  7,  7,  6, 16, 16, 16,  7,\n",
       "       16,  7,  7, 16,  7, 16, 16, 16, 16, 16, 16,  7, 16,  7,  7,  7, 16,\n",
       "       16, 16,  6,  6,  7,  7, 16,  6,  6, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        7, 16, 16,  7, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,  6, 16, 16,\n",
       "       16, 16,  7, 16, 16, 16, 16,  7,  7,  7,  6,  7, 16,  7, 16,  6, 16,\n",
       "        7, 16, 16,  7, 16, 16, 16,  7,  6, 16, 16, 16, 16, 16,  7, 16,  6,\n",
       "       16, 16, 16, 16, 16, 16, 16, 16, 16,  6, 16, 16,  6, 16, 16,  7,  7,\n",
       "       16, 16,  7,  6, 16,  7, 16, 16, 16, 16, 16,  7, 16,  7,  7, 16, 16,\n",
       "        7, 16, 16, 16, 16, 16,  7, 16, 16,  7, 16, 16, 16, 16, 16, 16, 16,\n",
       "       16, 16,  7, 16, 16, 16, 16, 16, 16,  6,  7, 16, 16, 16,  7, 16, 16,\n",
       "        7, 16, 16, 16, 16,  7, 16,  7, 16, 16,  7, 16, 16, 16,  7,  7, 16,\n",
       "       16,  6, 16,  7, 16, 16, 16, 16,  7, 16,  7, 16, 16, 16, 16,  7, 16,\n",
       "       16, 16, 16, 16,  7,  6, 16,  7,  6, 16, 16, 16,  7,  7, 16, 16,  7,\n",
       "        7, 16, 16, 16,  7, 16,  7,  7,  7,  6, 16,  7, 16, 16,  7,  7,  7,\n",
       "       16, 16, 16,  7,  6, 16, 16, 16,  7,  7, 16, 16, 16, 16,  7, 16,  7,\n",
       "       16,  7,  7, 16, 16,  7,  7, 16, 16, 16,  7, 16, 16,  7, 16, 16, 16,\n",
       "       16, 16, 16, 16, 16,  7, 16, 16, 16, 16, 16, 16, 16, 16, 16,  6, 16,\n",
       "       16, 16,  7, 16,  7,  6, 16, 16,  6, 16, 16,  7,  7,  7, 16, 16, 16,\n",
       "        6,  6,  7,  7,  7, 16, 16,  7,  7, 16,  6], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predecir los datos y guardarlos en el archivo\n",
    "\n",
    "predict = reg.predict(vectors)\n",
    "predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c14add2-9422-43f1-aad5-c9d14413b021",
   "metadata": {},
   "source": [
    "## 4. Persistencia del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "070dbb46-6931-4d46-a563-2ee5b95d1f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd5bcd6f-717b-4aa8-87d0-367249b59215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ModeloODS.joblib']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"ModeloODS.joblib\"\n",
    "dump(pipeline, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
